# BrightnESS NeXus File Writer

November 2016: This is a design study for how the thing could operate.

## Requirements and Project Status

The requirement is to store tons of meta data as supplied by the ECP and streams of data generated by detectors, choppers and other sources into NeXus files. In the streaming part, we have to accommodate high data rates.

At the NIAC 2016 streaming data was discussed and a file structure was agreed upon.

At a recent BrightnESS meeting it was agreed to get the file writer to work in principle, more or less single threaded, first and then to make it go fast. It was also agreed to transfer the meta data and the instructions about how to stream as json data.

## Design Overview

The initial design is a tale of two actors, the **FileMaster** and the **StreamMaster** and of an abstraction, the **Streamer**.

Actor as in the sense of the distributed actor pattern: This is a SW component which has a message queue which is checked at regular intervals from the main loop. The actor responds to messages on the queue and does things. If a need for communication arises, an actor may post message to other message queues.

### The Streamer

The streamer is a class which is responsible for writing one stream of data. To this purpose it holds handles to the appropriate (Kafka) Stream to write and the HDF5 handles for the datasets and data types to write. Its abstract interface will hold three main methods:

- **recv** This methods responsibility is to check for new data on the stream and if it finds data, to write it.
- **recv(StreamPointer start, StreamPointer end)** This is
for the use case of rerunning data file writing. It writes stream data form start to end.
- **closeStream()** for closing all connections and handles when streaming finishes.

Initializing streams and handles ought to happen in the constructor. As we have different streams, there will be different implementations of the Streamer class.

### The FileMaster

The FileMasters task is to coordinate data writing and streaming. To this purpose it will listen to the Kafka command stream and act on two main messages:

- The start streaming message
- The end streaming message

The FileMaster will be at any time in one of two modes: a **ready**  mode and a **streaming** mode. The latter is the mode the system is in when data is being acquired.

And this is how the FileMaster is supposed to behave in response to these two messages.

#### The Start Streaming Message

The start streaming message will contain the json data needed for the construction of the file. The response of the FileMaster to this message of course depends on the mode: In streaming mode, the message is rejected and an error is sent.

The reaction in ready mode is more interesting:

1. A new NeXus file is opened and filled according to the json configuration transmitted as part of the start streaming message. During the json parsing process appropriate Streamer classes are generated and entered into a list or vector. This whole process should be resilient: i.e. if there are errors of
any kind, report them but try to write as much data as possible. The only reason to really give up is when the file system is full.
2. With the list of Streamers as a parameter a StreamMaster is created and run in a separate thread.
3. The mode switches to streaming
4. A streaming started confirmation message is sent to the command stream

#### The Streaming Stop Message

In ready mode the FileMaster will ignore this message. May be log the erroneous use of this message.

In streaming mode it will cause the following things to happen:

1. The message will be forwarded to the StreamMaster.
2. Then the FileMaster waits for the StreamMaster to finish
The FileMaster may apply some Streaming Stop processing and finally close the file.
3. The FileMaster transitions into the ready state
4. A Streaming Stop confirmation message is sent to the command stream

### The StreamMaster

The StreamMaster is responsible for doing all the data streaming. To this purpose it has been initialized with a list of Streamer classes. In its main function it will do the following things:

1.) Iterate over all Streamers and call the writeStream() method.
2.) Check for the Stop Streaming message from the FileMaster and if this is received:
     1. Iterate over all Streamers and call the closeStream() method
  	 2. Terminate
else:
  Flush the NeXus file

It could be argued to run each Streamer in its own thread. But the HDF5 library implements thread safety by having one fat library lock which prevents access to the library by more then one thread. Thus nothing can be gained from a multithreaded implementation here. Only if we go for a parallel file system, we could use multiple process. This then will be another implementation of the StreamMaster, the ParallelStreamMaster. Deferred for now, but keep in mind that there may be different implementations of the StreamMaster.


## Data Management

Depending on the implementation the **Streamer** class may need to buffer stream data.

One use case where this may be necessary is when the streaming API works threaded and with callbacks. I.e. the streaming API sends data asynchronously. Then the Streamer has to buffer that data until the next call to writeStream().

With the NeXus file writer we ought to aim for a consistent and valid HDF5 file as far as possible. But HDF5 files are only guaranteed to be consistent when they are either closed or flushed. Or when the HDF5 journaling option has been enabled. But the latter causes double writing and thus looses performance. Flushing also imposes a performance penalty. Disk writes are involved. Thus flushing in each loop of the StreamMaster may not perform well. May be at regular time or after a number of pulses may be better. This is an area where only experimentation can give us answers.


## Support Classes

The two modes of the FileMaster can be implemented as separate state classes using a OO state machine pattern. Makes sense.

The FileMaster can benefit from a json parsing class plus associated helper classes. Though the cohesion in that class  would only be functional. This is more a module rather then a class.

A means is needed to write log messages and error messages. For simplicity, I think this should be written directly to appropriate Kafka streams. But a helper class to manage this is in order.  

Another complication is that one Kafka stream may contain data for more then one Streamer class. An example would be the EPICS-Kafka stream. In that case it would be stupid to process the stream more then once. Then a StreamMultiplexer (the name may be shit) is needed. It would register all child Streamers. It would process the stream and forward data as appropriate to child Streamers who buffer them until it is time for them to write it.


## Rerun Data File Writing

As all the data is buffered in Kafka, we have the option to rerun data file writing if something goes shitty. IMHO, this should be a separate application. It will share a lot of code with the live NeXus file writer. This application will be called with three parameters:

- A json description of the file to write
- A start and end pointer (probably timestamps) which define the section of the streams to write.

The algorithm will go like this:

1. Parse the json and write the meta data. Thereby initialize
Streamers and store them in a list as the FileMaster does.
2. Iterate across the list of Streamers and call writeStream(start,end) and then closeStream() on each of them.
3. close the file


## Kafka Management

When the data is in a valid HDF5 file, the Kafka data can be released. This is particularly important for the neutron event data. I think that this task can be offloaded to a separate program which listens to file writing messages, verifies files and reruns file writing or releases Kafka memory as appropriate.
Managing Kafka data is a different concern then writing the file.

## TODO
